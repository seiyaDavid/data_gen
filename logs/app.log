2025-01-30 19:42:20,743 - INFO - Starting synthetic data generation application.
2025-01-30 19:42:20,743 - INFO - Starting synthetic data generation process...
2025-01-30 19:42:20,744 - INFO - Loading and preprocessing data...
2025-01-30 19:42:20,744 - INFO - Loading data from: C:/Users/Seiya/Desktop/SyntheticImproved/synthetic_data_generator/data/original_data.csv
2025-01-30 19:42:20,746 - INFO - Data loaded successfully. Shape: (6, 6)
2025-01-30 19:42:20,746 - INFO - Original Data (First 5 Rows):
2025-01-30 19:42:20,746 - INFO -    numerical_column date_column categorical_column                  names        cost     ID
0                 2  2023-01-01                  B  Natalie Hutchinson MD  355.036652  A1095
1                 1  2023-01-01                  A         Willie Johnson  200.449997  C3907
2                 2  2023-01-01                  A           John Sampson  200.449997  Z1095
3                 1  2023-01-01                  A         Gina Hernandez  200.449997  A0095
4                 3  2023-01-01                  A        Richard Elliott  200.449997  P6095
2025-01-30 19:42:20,818 - INFO - Column 'numerical_column' detected as numerical.
2025-01-30 19:42:20,820 - INFO - Column 'date_column' detected as name/categorical.
2025-01-30 19:42:20,821 - INFO - Column 'categorical_column' detected as name/categorical.
2025-01-30 19:42:20,821 - INFO - Column 'names' detected as the name column.
2025-01-30 19:42:20,821 - INFO - Column 'cost' detected as numerical.
2025-01-30 19:42:20,822 - INFO - Column 'ID' detected as alphanumeric.
2025-01-30 19:42:20,824 - INFO - Fitted MinMaxScaler for numerical column: numerical_column
2025-01-30 19:42:20,825 - INFO - Fitted MinMaxScaler for numerical column: cost
2025-01-30 19:42:20,825 - INFO - Fitted LabelEncoder for name/categorical column: date_column
2025-01-30 19:42:20,825 - INFO - Fitted LabelEncoder for name/categorical column: categorical_column
2025-01-30 19:42:20,830 - INFO - Columns in df_transformed: Index(['numerical_column', 'date_column', 'categorical_column', 'names',
       'cost', 'ID_letter', 'ID_number'],
      dtype='object')
2025-01-30 19:42:20,831 - INFO - Processed Data (First 5 Rows):
2025-01-30 19:42:20,831 - INFO -    numerical_column  date_column  categorical_column                  names  cost  ID_letter  ID_number
0               0.5            0                   1  Natalie Hutchinson MD   1.0          0   0.114943
1               0.0            0                   0         Willie Johnson   0.0          1   0.438161
2               0.5            0                   0           John Sampson   0.0          4   0.114943
3               0.0            0                   0         Gina Hernandez   0.0          0   0.000000
4               1.0            0                   0        Richard Elliott   0.0          3   0.689655
2025-01-30 19:42:20,843 - INFO - Training GAN model...
2025-01-30 19:42:21,880 - INFO - Starting GAN training...
2025-01-30 19:42:21,912 - INFO - Epoch [0/300], Batch [0/1], Loss D: 1.3949, Loss G: 0.7043
2025-01-30 19:42:21,926 - INFO - Epoch [1/300], Batch [0/1], Loss D: 1.3973, Loss G: 0.7012
2025-01-30 19:42:21,939 - INFO - Epoch [2/300], Batch [0/1], Loss D: 1.3906, Loss G: 0.7074
2025-01-30 19:42:21,952 - INFO - Epoch [3/300], Batch [0/1], Loss D: 1.3871, Loss G: 0.7099
2025-01-30 19:42:21,966 - INFO - Epoch [4/300], Batch [0/1], Loss D: 1.3985, Loss G: 0.6978
2025-01-30 19:42:21,977 - INFO - Epoch [5/300], Batch [0/1], Loss D: 1.3911, Loss G: 0.7044
2025-01-30 19:42:21,991 - INFO - Epoch [6/300], Batch [0/1], Loss D: 1.3865, Loss G: 0.7084
2025-01-30 19:42:22,005 - INFO - Epoch [7/300], Batch [0/1], Loss D: 1.3905, Loss G: 0.7036
2025-01-30 19:42:22,019 - INFO - Epoch [8/300], Batch [0/1], Loss D: 1.3919, Loss G: 0.7017
2025-01-30 19:42:22,033 - INFO - Epoch [9/300], Batch [0/1], Loss D: 1.3839, Loss G: 0.7091
2025-01-30 19:42:22,046 - INFO - Epoch [10/300], Batch [0/1], Loss D: 1.3866, Loss G: 0.7059
2025-01-30 19:42:22,059 - INFO - Epoch [11/300], Batch [0/1], Loss D: 1.3846, Loss G: 0.7070
2025-01-30 19:42:22,071 - INFO - Epoch [12/300], Batch [0/1], Loss D: 1.3916, Loss G: 0.6991
2025-01-30 19:42:22,082 - INFO - Epoch [13/300], Batch [0/1], Loss D: 1.3888, Loss G: 0.7012
2025-01-30 19:42:22,095 - INFO - Epoch [14/300], Batch [0/1], Loss D: 1.3839, Loss G: 0.7061
2025-01-30 19:42:22,108 - INFO - Epoch [15/300], Batch [0/1], Loss D: 1.3883, Loss G: 0.7001
2025-01-30 19:42:22,121 - INFO - Epoch [16/300], Batch [0/1], Loss D: 1.3847, Loss G: 0.7033
2025-01-30 19:42:22,133 - INFO - Epoch [17/300], Batch [0/1], Loss D: 1.3871, Loss G: 0.7000
2025-01-30 19:42:22,149 - INFO - Epoch [18/300], Batch [0/1], Loss D: 1.3773, Loss G: 0.7096
2025-01-30 19:42:22,168 - INFO - Epoch [19/300], Batch [0/1], Loss D: 1.3808, Loss G: 0.7051
2025-01-30 19:42:22,184 - INFO - Epoch [20/300], Batch [0/1], Loss D: 1.3830, Loss G: 0.7021
2025-01-30 19:42:22,198 - INFO - Epoch [21/300], Batch [0/1], Loss D: 1.3788, Loss G: 0.7057
2025-01-30 19:42:22,214 - INFO - Epoch [22/300], Batch [0/1], Loss D: 1.3801, Loss G: 0.7037
2025-01-30 19:42:22,230 - INFO - Epoch [23/300], Batch [0/1], Loss D: 1.3769, Loss G: 0.7063
2025-01-30 19:42:22,246 - INFO - Epoch [24/300], Batch [0/1], Loss D: 1.3791, Loss G: 0.7034
2025-01-30 19:42:22,259 - INFO - Epoch [25/300], Batch [0/1], Loss D: 1.3752, Loss G: 0.7067
2025-01-30 19:42:22,272 - INFO - Epoch [26/300], Batch [0/1], Loss D: 1.3771, Loss G: 0.7041
2025-01-30 19:42:22,289 - INFO - Epoch [27/300], Batch [0/1], Loss D: 1.3767, Loss G: 0.7037
2025-01-30 19:42:22,303 - INFO - Epoch [28/300], Batch [0/1], Loss D: 1.3792, Loss G: 0.7005
2025-01-30 19:42:22,315 - INFO - Epoch [29/300], Batch [0/1], Loss D: 1.3734, Loss G: 0.7057
2025-01-30 19:42:22,328 - INFO - Epoch [30/300], Batch [0/1], Loss D: 1.3695, Loss G: 0.7092
2025-01-30 19:42:22,341 - INFO - Epoch [31/300], Batch [0/1], Loss D: 1.3790, Loss G: 0.6986
2025-01-30 19:42:22,354 - INFO - Epoch [32/300], Batch [0/1], Loss D: 1.3748, Loss G: 0.7023
2025-01-30 19:42:22,368 - INFO - Epoch [33/300], Batch [0/1], Loss D: 1.3688, Loss G: 0.7079
2025-01-30 19:42:22,382 - INFO - Epoch [34/300], Batch [0/1], Loss D: 1.3713, Loss G: 0.7044
2025-01-30 19:42:22,396 - INFO - Epoch [35/300], Batch [0/1], Loss D: 1.3765, Loss G: 0.6986
2025-01-30 19:42:22,412 - INFO - Epoch [36/300], Batch [0/1], Loss D: 1.3684, Loss G: 0.7062
2025-01-30 19:42:22,427 - INFO - Epoch [37/300], Batch [0/1], Loss D: 1.3713, Loss G: 0.7028
2025-01-30 19:42:22,443 - INFO - Epoch [38/300], Batch [0/1], Loss D: 1.3673, Loss G: 0.7061
2025-01-30 19:42:22,458 - INFO - Epoch [39/300], Batch [0/1], Loss D: 1.3641, Loss G: 0.7089
2025-01-30 19:42:22,476 - INFO - Epoch [40/300], Batch [0/1], Loss D: 1.3726, Loss G: 0.6992
2025-01-30 19:42:22,493 - INFO - Epoch [41/300], Batch [0/1], Loss D: 1.3655, Loss G: 0.7059
2025-01-30 19:42:22,510 - INFO - Epoch [42/300], Batch [0/1], Loss D: 1.3658, Loss G: 0.7048
2025-01-30 19:42:22,527 - INFO - Epoch [43/300], Batch [0/1], Loss D: 1.3617, Loss G: 0.7085
2025-01-30 19:42:22,544 - INFO - Epoch [44/300], Batch [0/1], Loss D: 1.3679, Loss G: 0.7022
2025-01-30 19:42:22,559 - INFO - Epoch [45/300], Batch [0/1], Loss D: 1.3629, Loss G: 0.7060
2025-01-30 19:42:22,573 - INFO - Epoch [46/300], Batch [0/1], Loss D: 1.3630, Loss G: 0.7055
2025-01-30 19:42:22,587 - INFO - Epoch [47/300], Batch [0/1], Loss D: 1.3595, Loss G: 0.7086
2025-01-30 19:42:22,601 - INFO - Epoch [48/300], Batch [0/1], Loss D: 1.3647, Loss G: 0.7025
2025-01-30 19:42:22,616 - INFO - Epoch [49/300], Batch [0/1], Loss D: 1.3642, Loss G: 0.7020
2025-01-30 19:42:22,631 - INFO - Epoch [50/300], Batch [0/1], Loss D: 1.3689, Loss G: 0.6973
2025-01-30 19:42:22,646 - INFO - Epoch [51/300], Batch [0/1], Loss D: 1.3588, Loss G: 0.7065
2025-01-30 19:42:22,667 - INFO - Epoch [52/300], Batch [0/1], Loss D: 1.3549, Loss G: 0.7099
2025-01-30 19:42:22,682 - INFO - Epoch [53/300], Batch [0/1], Loss D: 1.3546, Loss G: 0.7096
2025-01-30 19:42:22,699 - INFO - Epoch [54/300], Batch [0/1], Loss D: 1.3535, Loss G: 0.7100
2025-01-30 19:42:22,714 - INFO - Epoch [55/300], Batch [0/1], Loss D: 1.3532, Loss G: 0.7099
2025-01-30 19:42:22,730 - INFO - Epoch [56/300], Batch [0/1], Loss D: 1.3583, Loss G: 0.7036
2025-01-30 19:42:22,745 - INFO - Epoch [57/300], Batch [0/1], Loss D: 1.3508, Loss G: 0.7111
2025-01-30 19:42:22,759 - INFO - Epoch [58/300], Batch [0/1], Loss D: 1.3502, Loss G: 0.7109
2025-01-30 19:42:22,774 - INFO - Epoch [59/300], Batch [0/1], Loss D: 1.3498, Loss G: 0.7111
2025-01-30 19:42:22,786 - INFO - Epoch [60/300], Batch [0/1], Loss D: 1.3510, Loss G: 0.7090
2025-01-30 19:42:22,800 - INFO - Epoch [61/300], Batch [0/1], Loss D: 1.3558, Loss G: 0.7033
2025-01-30 19:42:22,814 - INFO - Epoch [62/300], Batch [0/1], Loss D: 1.3476, Loss G: 0.7110
2025-01-30 19:42:22,828 - INFO - Epoch [63/300], Batch [0/1], Loss D: 1.3425, Loss G: 0.7161
2025-01-30 19:42:22,842 - INFO - Epoch [64/300], Batch [0/1], Loss D: 1.3532, Loss G: 0.7042
2025-01-30 19:42:22,856 - INFO - Epoch [65/300], Batch [0/1], Loss D: 1.3466, Loss G: 0.7109
2025-01-30 19:42:22,869 - INFO - Epoch [66/300], Batch [0/1], Loss D: 1.3522, Loss G: 0.7039
2025-01-30 19:42:22,882 - INFO - Epoch [67/300], Batch [0/1], Loss D: 1.3547, Loss G: 0.7006
2025-01-30 19:42:22,894 - INFO - Epoch [68/300], Batch [0/1], Loss D: 1.3458, Loss G: 0.7093
2025-01-30 19:42:22,907 - INFO - Epoch [69/300], Batch [0/1], Loss D: 1.3407, Loss G: 0.7144
2025-01-30 19:42:22,919 - INFO - Epoch [70/300], Batch [0/1], Loss D: 1.3536, Loss G: 0.7007
2025-01-30 19:42:22,931 - INFO - Epoch [71/300], Batch [0/1], Loss D: 1.3392, Loss G: 0.7147
2025-01-30 19:42:22,945 - INFO - Epoch [72/300], Batch [0/1], Loss D: 1.3444, Loss G: 0.7087
2025-01-30 19:42:22,958 - INFO - Epoch [73/300], Batch [0/1], Loss D: 1.3470, Loss G: 0.7052
2025-01-30 19:42:22,970 - INFO - Epoch [74/300], Batch [0/1], Loss D: 1.3550, Loss G: 0.6967
2025-01-30 19:42:22,983 - INFO - Epoch [75/300], Batch [0/1], Loss D: 1.3383, Loss G: 0.7130
2025-01-30 19:42:22,995 - INFO - Epoch [76/300], Batch [0/1], Loss D: 1.3405, Loss G: 0.7100
2025-01-30 19:42:23,007 - INFO - Epoch [77/300], Batch [0/1], Loss D: 1.3359, Loss G: 0.7142
2025-01-30 19:42:23,018 - INFO - Epoch [78/300], Batch [0/1], Loss D: 1.3323, Loss G: 0.7174
2025-01-30 19:42:23,028 - INFO - Epoch [79/300], Batch [0/1], Loss D: 1.3458, Loss G: 0.7026
2025-01-30 19:42:23,038 - INFO - Epoch [80/300], Batch [0/1], Loss D: 1.3460, Loss G: 0.7021
2025-01-30 19:42:23,049 - INFO - Epoch [81/300], Batch [0/1], Loss D: 1.3383, Loss G: 0.7098
2025-01-30 19:42:23,059 - INFO - Epoch [82/300], Batch [0/1], Loss D: 1.3305, Loss G: 0.7181
2025-01-30 19:42:23,069 - INFO - Epoch [83/300], Batch [0/1], Loss D: 1.3365, Loss G: 0.7101
2025-01-30 19:42:23,079 - INFO - Epoch [84/300], Batch [0/1], Loss D: 1.3289, Loss G: 0.7175
2025-01-30 19:42:23,090 - INFO - Epoch [85/300], Batch [0/1], Loss D: 1.3311, Loss G: 0.7159
2025-01-30 19:42:23,100 - INFO - Epoch [86/300], Batch [0/1], Loss D: 1.3324, Loss G: 0.7126
2025-01-30 19:42:23,111 - INFO - Epoch [87/300], Batch [0/1], Loss D: 1.3293, Loss G: 0.7155
2025-01-30 19:42:23,121 - INFO - Epoch [88/300], Batch [0/1], Loss D: 1.3275, Loss G: 0.7169
2025-01-30 19:42:23,132 - INFO - Epoch [89/300], Batch [0/1], Loss D: 1.3325, Loss G: 0.7113
2025-01-30 19:42:23,142 - INFO - Epoch [90/300], Batch [0/1], Loss D: 1.3296, Loss G: 0.7132
2025-01-30 19:42:23,152 - INFO - Epoch [91/300], Batch [0/1], Loss D: 1.3258, Loss G: 0.7167
2025-01-30 19:42:23,162 - INFO - Epoch [92/300], Batch [0/1], Loss D: 1.3277, Loss G: 0.7144
2025-01-30 19:42:23,173 - INFO - Epoch [93/300], Batch [0/1], Loss D: 1.3274, Loss G: 0.7141
2025-01-30 19:42:23,183 - INFO - Epoch [94/300], Batch [0/1], Loss D: 1.3400, Loss G: 0.7008
2025-01-30 19:42:23,194 - INFO - Epoch [95/300], Batch [0/1], Loss D: 1.3218, Loss G: 0.7186
2025-01-30 19:42:23,204 - INFO - Epoch [96/300], Batch [0/1], Loss D: 1.3287, Loss G: 0.7112
2025-01-30 19:42:23,214 - INFO - Epoch [97/300], Batch [0/1], Loss D: 1.3197, Loss G: 0.7197
2025-01-30 19:42:23,225 - INFO - Epoch [98/300], Batch [0/1], Loss D: 1.3331, Loss G: 0.7052
2025-01-30 19:42:23,235 - INFO - Epoch [99/300], Batch [0/1], Loss D: 1.3299, Loss G: 0.7081
2025-01-30 19:42:23,245 - INFO - Epoch [100/300], Batch [0/1], Loss D: 1.3291, Loss G: 0.7084
2025-01-30 19:42:23,256 - INFO - Epoch [101/300], Batch [0/1], Loss D: 1.3331, Loss G: 0.7049
2025-01-30 19:42:23,266 - INFO - Epoch [102/300], Batch [0/1], Loss D: 1.3292, Loss G: 0.7077
2025-01-30 19:42:23,276 - INFO - Epoch [103/300], Batch [0/1], Loss D: 1.3195, Loss G: 0.7167
2025-01-30 19:42:23,287 - INFO - Epoch [104/300], Batch [0/1], Loss D: 1.3300, Loss G: 0.7052
2025-01-30 19:42:23,297 - INFO - Epoch [105/300], Batch [0/1], Loss D: 1.3288, Loss G: 0.7077
2025-01-30 19:42:23,313 - INFO - Epoch [106/300], Batch [0/1], Loss D: 1.3236, Loss G: 0.7111
2025-01-30 19:42:23,325 - INFO - Epoch [107/300], Batch [0/1], Loss D: 1.3190, Loss G: 0.7151
2025-01-30 19:42:23,338 - INFO - Epoch [108/300], Batch [0/1], Loss D: 1.3232, Loss G: 0.7102
2025-01-30 19:42:23,353 - INFO - Epoch [109/300], Batch [0/1], Loss D: 1.3191, Loss G: 0.7141
2025-01-30 19:42:23,368 - INFO - Epoch [110/300], Batch [0/1], Loss D: 1.3194, Loss G: 0.7132
2025-01-30 19:42:23,378 - INFO - Epoch [111/300], Batch [0/1], Loss D: 1.3204, Loss G: 0.7121
2025-01-30 19:42:23,388 - INFO - Epoch [112/300], Batch [0/1], Loss D: 1.3167, Loss G: 0.7155
2025-01-30 19:42:23,398 - INFO - Epoch [113/300], Batch [0/1], Loss D: 1.3126, Loss G: 0.7197
2025-01-30 19:42:23,409 - INFO - Epoch [114/300], Batch [0/1], Loss D: 1.3196, Loss G: 0.7117
2025-01-30 19:42:23,418 - INFO - Epoch [115/300], Batch [0/1], Loss D: 1.3189, Loss G: 0.7121
2025-01-30 19:42:23,430 - INFO - Epoch [116/300], Batch [0/1], Loss D: 1.3138, Loss G: 0.7159
2025-01-30 19:42:23,440 - INFO - Epoch [117/300], Batch [0/1], Loss D: 1.3152, Loss G: 0.7139
2025-01-30 19:42:23,450 - INFO - Epoch [118/300], Batch [0/1], Loss D: 1.3199, Loss G: 0.7101
2025-01-30 19:42:23,460 - INFO - Epoch [119/300], Batch [0/1], Loss D: 1.3147, Loss G: 0.7135
2025-01-30 19:42:23,470 - INFO - Epoch [120/300], Batch [0/1], Loss D: 1.3081, Loss G: 0.7199
2025-01-30 19:42:23,480 - INFO - Epoch [121/300], Batch [0/1], Loss D: 1.3073, Loss G: 0.7204
2025-01-30 19:42:23,490 - INFO - Epoch [122/300], Batch [0/1], Loss D: 1.3220, Loss G: 0.7053
2025-01-30 19:42:23,500 - INFO - Epoch [123/300], Batch [0/1], Loss D: 1.3098, Loss G: 0.7171
2025-01-30 19:42:23,511 - INFO - Epoch [124/300], Batch [0/1], Loss D: 1.3158, Loss G: 0.7098
2025-01-30 19:42:23,522 - INFO - Epoch [125/300], Batch [0/1], Loss D: 1.3156, Loss G: 0.7106
2025-01-30 19:42:23,532 - INFO - Epoch [126/300], Batch [0/1], Loss D: 1.3090, Loss G: 0.7161
2025-01-30 19:42:23,543 - INFO - Epoch [127/300], Batch [0/1], Loss D: 1.3089, Loss G: 0.7159
2025-01-30 19:42:23,553 - INFO - Epoch [128/300], Batch [0/1], Loss D: 1.3086, Loss G: 0.7153
2025-01-30 19:42:23,563 - INFO - Epoch [129/300], Batch [0/1], Loss D: 1.3033, Loss G: 0.7207
2025-01-30 19:42:23,573 - INFO - Epoch [130/300], Batch [0/1], Loss D: 1.2946, Loss G: 0.7293
2025-01-30 19:42:23,583 - INFO - Epoch [131/300], Batch [0/1], Loss D: 1.3132, Loss G: 0.7101
2025-01-30 19:42:23,593 - INFO - Epoch [132/300], Batch [0/1], Loss D: 1.3104, Loss G: 0.7123
2025-01-30 19:42:23,604 - INFO - Epoch [133/300], Batch [0/1], Loss D: 1.3072, Loss G: 0.7148
2025-01-30 19:42:23,614 - INFO - Epoch [134/300], Batch [0/1], Loss D: 1.3131, Loss G: 0.7078
2025-01-30 19:42:23,625 - INFO - Epoch [135/300], Batch [0/1], Loss D: 1.3158, Loss G: 0.7057
2025-01-30 19:42:23,634 - INFO - Epoch [136/300], Batch [0/1], Loss D: 1.3004, Loss G: 0.7205
2025-01-30 19:42:23,645 - INFO - Epoch [137/300], Batch [0/1], Loss D: 1.3305, Loss G: 0.6919
2025-01-30 19:42:23,655 - INFO - Epoch [138/300], Batch [0/1], Loss D: 1.2969, Loss G: 0.7231
2025-01-30 19:42:23,667 - INFO - Epoch [139/300], Batch [0/1], Loss D: 1.3153, Loss G: 0.7037
2025-01-30 19:42:23,679 - INFO - Epoch [140/300], Batch [0/1], Loss D: 1.3013, Loss G: 0.7173
2025-01-30 19:42:23,690 - INFO - Epoch [141/300], Batch [0/1], Loss D: 1.3091, Loss G: 0.7083
2025-01-30 19:42:23,701 - INFO - Epoch [142/300], Batch [0/1], Loss D: 1.2961, Loss G: 0.7219
2025-01-30 19:42:23,711 - INFO - Epoch [143/300], Batch [0/1], Loss D: 1.2904, Loss G: 0.7276
2025-01-30 19:42:23,722 - INFO - Epoch [144/300], Batch [0/1], Loss D: 1.2867, Loss G: 0.7305
2025-01-30 19:42:23,731 - INFO - Epoch [145/300], Batch [0/1], Loss D: 1.2934, Loss G: 0.7230
2025-01-30 19:42:23,742 - INFO - Epoch [146/300], Batch [0/1], Loss D: 1.2984, Loss G: 0.7181
2025-01-30 19:42:23,753 - INFO - Epoch [147/300], Batch [0/1], Loss D: 1.3031, Loss G: 0.7130
2025-01-30 19:42:23,765 - INFO - Epoch [148/300], Batch [0/1], Loss D: 1.3037, Loss G: 0.7113
2025-01-30 19:42:23,776 - INFO - Epoch [149/300], Batch [0/1], Loss D: 1.2899, Loss G: 0.7247
2025-01-30 19:42:23,786 - INFO - Epoch [150/300], Batch [0/1], Loss D: 1.3019, Loss G: 0.7123
2025-01-30 19:42:23,799 - INFO - Epoch [151/300], Batch [0/1], Loss D: 1.3004, Loss G: 0.7133
2025-01-30 19:42:23,812 - INFO - Epoch [152/300], Batch [0/1], Loss D: 1.2963, Loss G: 0.7166
2025-01-30 19:42:23,824 - INFO - Epoch [153/300], Batch [0/1], Loss D: 1.3000, Loss G: 0.7125
2025-01-30 19:42:23,836 - INFO - Epoch [154/300], Batch [0/1], Loss D: 1.3188, Loss G: 0.6941
2025-01-30 19:42:23,848 - INFO - Epoch [155/300], Batch [0/1], Loss D: 1.2820, Loss G: 0.7309
2025-01-30 19:42:23,861 - INFO - Epoch [156/300], Batch [0/1], Loss D: 1.2965, Loss G: 0.7145
2025-01-30 19:42:23,874 - INFO - Epoch [157/300], Batch [0/1], Loss D: 1.2941, Loss G: 0.7165
2025-01-30 19:42:23,887 - INFO - Epoch [158/300], Batch [0/1], Loss D: 1.2980, Loss G: 0.7147
2025-01-30 19:42:23,899 - INFO - Epoch [159/300], Batch [0/1], Loss D: 1.2896, Loss G: 0.7217
2025-01-30 19:42:23,911 - INFO - Epoch [160/300], Batch [0/1], Loss D: 1.3019, Loss G: 0.7083
2025-01-30 19:42:23,924 - INFO - Epoch [161/300], Batch [0/1], Loss D: 1.3040, Loss G: 0.7065
2025-01-30 19:42:23,935 - INFO - Epoch [162/300], Batch [0/1], Loss D: 1.2929, Loss G: 0.7164
2025-01-30 19:42:23,947 - INFO - Epoch [163/300], Batch [0/1], Loss D: 1.2981, Loss G: 0.7101
2025-01-30 19:42:23,959 - INFO - Epoch [164/300], Batch [0/1], Loss D: 1.2814, Loss G: 0.7282
2025-01-30 19:42:23,971 - INFO - Epoch [165/300], Batch [0/1], Loss D: 1.2882, Loss G: 0.7202
2025-01-30 19:42:23,982 - INFO - Epoch [166/300], Batch [0/1], Loss D: 1.2824, Loss G: 0.7250
2025-01-30 19:42:23,994 - INFO - Epoch [167/300], Batch [0/1], Loss D: 1.2863, Loss G: 0.7212
2025-01-30 19:42:24,006 - INFO - Epoch [168/300], Batch [0/1], Loss D: 1.2851, Loss G: 0.7218
2025-01-30 19:42:24,018 - INFO - Epoch [169/300], Batch [0/1], Loss D: 1.2975, Loss G: 0.7090
2025-01-30 19:42:24,031 - INFO - Epoch [170/300], Batch [0/1], Loss D: 1.2934, Loss G: 0.7134
2025-01-30 19:42:24,043 - INFO - Epoch [171/300], Batch [0/1], Loss D: 1.2913, Loss G: 0.7139
2025-01-30 19:42:24,055 - INFO - Epoch [172/300], Batch [0/1], Loss D: 1.2825, Loss G: 0.7222
2025-01-30 19:42:24,065 - INFO - Epoch [173/300], Batch [0/1], Loss D: 1.3030, Loss G: 0.7019
2025-01-30 19:42:24,076 - INFO - Epoch [174/300], Batch [0/1], Loss D: 1.2922, Loss G: 0.7122
2025-01-30 19:42:24,087 - INFO - Epoch [175/300], Batch [0/1], Loss D: 1.2989, Loss G: 0.7045
2025-01-30 19:42:24,097 - INFO - Epoch [176/300], Batch [0/1], Loss D: 1.2860, Loss G: 0.7174
2025-01-30 19:42:24,108 - INFO - Epoch [177/300], Batch [0/1], Loss D: 1.2883, Loss G: 0.7152
2025-01-30 19:42:24,118 - INFO - Epoch [178/300], Batch [0/1], Loss D: 1.3014, Loss G: 0.7026
2025-01-30 19:42:24,129 - INFO - Epoch [179/300], Batch [0/1], Loss D: 1.2850, Loss G: 0.7173
2025-01-30 19:42:24,141 - INFO - Epoch [180/300], Batch [0/1], Loss D: 1.2875, Loss G: 0.7155
2025-01-30 19:42:24,153 - INFO - Epoch [181/300], Batch [0/1], Loss D: 1.2847, Loss G: 0.7168
2025-01-30 19:42:24,165 - INFO - Epoch [182/300], Batch [0/1], Loss D: 1.2874, Loss G: 0.7139
2025-01-30 19:42:24,176 - INFO - Epoch [183/300], Batch [0/1], Loss D: 1.2955, Loss G: 0.7055
2025-01-30 19:42:24,186 - INFO - Epoch [184/300], Batch [0/1], Loss D: 1.2722, Loss G: 0.7286
2025-01-30 19:42:24,197 - INFO - Epoch [185/300], Batch [0/1], Loss D: 1.2876, Loss G: 0.7125
2025-01-30 19:42:24,207 - INFO - Epoch [186/300], Batch [0/1], Loss D: 1.2943, Loss G: 0.7057
2025-01-30 19:42:24,218 - INFO - Epoch [187/300], Batch [0/1], Loss D: 1.2680, Loss G: 0.7317
2025-01-30 19:42:24,228 - INFO - Epoch [188/300], Batch [0/1], Loss D: 1.2912, Loss G: 0.7093
2025-01-30 19:42:24,237 - INFO - Epoch [189/300], Batch [0/1], Loss D: 1.2992, Loss G: 0.6998
2025-01-30 19:42:24,248 - INFO - Epoch [190/300], Batch [0/1], Loss D: 1.2686, Loss G: 0.7300
2025-01-30 19:42:24,258 - INFO - Epoch [191/300], Batch [0/1], Loss D: 1.2827, Loss G: 0.7143
2025-01-30 19:42:24,268 - INFO - Epoch [192/300], Batch [0/1], Loss D: 1.3009, Loss G: 0.6974
2025-01-30 19:42:24,278 - INFO - Epoch [193/300], Batch [0/1], Loss D: 1.2712, Loss G: 0.7260
2025-01-30 19:42:24,289 - INFO - Epoch [194/300], Batch [0/1], Loss D: 1.2654, Loss G: 0.7326
2025-01-30 19:42:24,299 - INFO - Epoch [195/300], Batch [0/1], Loss D: 1.2839, Loss G: 0.7123
2025-01-30 19:42:24,311 - INFO - Epoch [196/300], Batch [0/1], Loss D: 1.2791, Loss G: 0.7176
2025-01-30 19:42:24,328 - INFO - Epoch [197/300], Batch [0/1], Loss D: 1.2742, Loss G: 0.7210
2025-01-30 19:42:24,346 - INFO - Epoch [198/300], Batch [0/1], Loss D: 1.2876, Loss G: 0.7102
2025-01-30 19:42:24,358 - INFO - Epoch [199/300], Batch [0/1], Loss D: 1.2643, Loss G: 0.7308
2025-01-30 19:42:24,368 - INFO - Epoch [200/300], Batch [0/1], Loss D: 1.2703, Loss G: 0.7260
2025-01-30 19:42:24,378 - INFO - Epoch [201/300], Batch [0/1], Loss D: 1.2882, Loss G: 0.7074
2025-01-30 19:42:24,389 - INFO - Epoch [202/300], Batch [0/1], Loss D: 1.2705, Loss G: 0.7235
2025-01-30 19:42:24,399 - INFO - Epoch [203/300], Batch [0/1], Loss D: 1.2777, Loss G: 0.7158
2025-01-30 19:42:24,409 - INFO - Epoch [204/300], Batch [0/1], Loss D: 1.2765, Loss G: 0.7165
2025-01-30 19:42:24,419 - INFO - Epoch [205/300], Batch [0/1], Loss D: 1.2698, Loss G: 0.7255
2025-01-30 19:42:24,430 - INFO - Epoch [206/300], Batch [0/1], Loss D: 1.2762, Loss G: 0.7197
2025-01-30 19:42:24,441 - INFO - Epoch [207/300], Batch [0/1], Loss D: 1.2744, Loss G: 0.7180
2025-01-30 19:42:24,452 - INFO - Epoch [208/300], Batch [0/1], Loss D: 1.3005, Loss G: 0.6942
2025-01-30 19:42:24,463 - INFO - Epoch [209/300], Batch [0/1], Loss D: 1.2681, Loss G: 0.7249
2025-01-30 19:42:24,473 - INFO - Epoch [210/300], Batch [0/1], Loss D: 1.2611, Loss G: 0.7318
2025-01-30 19:42:24,483 - INFO - Epoch [211/300], Batch [0/1], Loss D: 1.2932, Loss G: 0.6988
2025-01-30 19:42:24,493 - INFO - Epoch [212/300], Batch [0/1], Loss D: 1.3014, Loss G: 0.6921
2025-01-30 19:42:24,503 - INFO - Epoch [213/300], Batch [0/1], Loss D: 1.2681, Loss G: 0.7220
2025-01-30 19:42:24,514 - INFO - Epoch [214/300], Batch [0/1], Loss D: 1.3236, Loss G: 0.6732
2025-01-30 19:42:24,524 - INFO - Epoch [215/300], Batch [0/1], Loss D: 1.2759, Loss G: 0.7149
2025-01-30 19:42:24,535 - INFO - Epoch [216/300], Batch [0/1], Loss D: 1.2709, Loss G: 0.7215
2025-01-30 19:42:24,546 - INFO - Epoch [217/300], Batch [0/1], Loss D: 1.2610, Loss G: 0.7282
2025-01-30 19:42:24,556 - INFO - Epoch [218/300], Batch [0/1], Loss D: 1.2791, Loss G: 0.7104
2025-01-30 19:42:24,566 - INFO - Epoch [219/300], Batch [0/1], Loss D: 1.2700, Loss G: 0.7186
2025-01-30 19:42:24,577 - INFO - Epoch [220/300], Batch [0/1], Loss D: 1.2785, Loss G: 0.7107
2025-01-30 19:42:24,586 - INFO - Epoch [221/300], Batch [0/1], Loss D: 1.2650, Loss G: 0.7230
2025-01-30 19:42:24,597 - INFO - Epoch [222/300], Batch [0/1], Loss D: 1.2650, Loss G: 0.7234
2025-01-30 19:42:24,609 - INFO - Epoch [223/300], Batch [0/1], Loss D: 1.2707, Loss G: 0.7159
2025-01-30 19:42:24,618 - INFO - Epoch [224/300], Batch [0/1], Loss D: 1.2898, Loss G: 0.6990
2025-01-30 19:42:24,629 - INFO - Epoch [225/300], Batch [0/1], Loss D: 1.2649, Loss G: 0.7251
2025-01-30 19:42:24,640 - INFO - Epoch [226/300], Batch [0/1], Loss D: 1.2685, Loss G: 0.7218
2025-01-30 19:42:24,649 - INFO - Epoch [227/300], Batch [0/1], Loss D: 1.2810, Loss G: 0.7063
2025-01-30 19:42:24,660 - INFO - Epoch [228/300], Batch [0/1], Loss D: 1.2611, Loss G: 0.7261
2025-01-30 19:42:24,670 - INFO - Epoch [229/300], Batch [0/1], Loss D: 1.2681, Loss G: 0.7203
2025-01-30 19:42:24,680 - INFO - Epoch [230/300], Batch [0/1], Loss D: 1.2726, Loss G: 0.7131
2025-01-30 19:42:24,693 - INFO - Epoch [231/300], Batch [0/1], Loss D: 1.2398, Loss G: 0.7460
2025-01-30 19:42:24,705 - INFO - Epoch [232/300], Batch [0/1], Loss D: 1.2915, Loss G: 0.6932
2025-01-30 19:42:24,715 - INFO - Epoch [233/300], Batch [0/1], Loss D: 1.2700, Loss G: 0.7131
2025-01-30 19:42:24,725 - INFO - Epoch [234/300], Batch [0/1], Loss D: 1.2692, Loss G: 0.7139
2025-01-30 19:42:24,735 - INFO - Epoch [235/300], Batch [0/1], Loss D: 1.2861, Loss G: 0.6979
2025-01-30 19:42:24,746 - INFO - Epoch [236/300], Batch [0/1], Loss D: 1.2588, Loss G: 0.7250
2025-01-30 19:42:24,758 - INFO - Epoch [237/300], Batch [0/1], Loss D: 1.2646, Loss G: 0.7179
2025-01-30 19:42:24,768 - INFO - Epoch [238/300], Batch [0/1], Loss D: 1.2870, Loss G: 0.7013
2025-01-30 19:42:24,778 - INFO - Epoch [239/300], Batch [0/1], Loss D: 1.2533, Loss G: 0.7295
2025-01-30 19:42:24,789 - INFO - Epoch [240/300], Batch [0/1], Loss D: 1.2601, Loss G: 0.7227
2025-01-30 19:42:24,799 - INFO - Epoch [241/300], Batch [0/1], Loss D: 1.2728, Loss G: 0.7106
2025-01-30 19:42:24,809 - INFO - Epoch [242/300], Batch [0/1], Loss D: 1.2823, Loss G: 0.7017
2025-01-30 19:42:24,822 - INFO - Epoch [243/300], Batch [0/1], Loss D: 1.2760, Loss G: 0.7103
2025-01-30 19:42:24,833 - INFO - Epoch [244/300], Batch [0/1], Loss D: 1.2835, Loss G: 0.6984
2025-01-30 19:42:24,845 - INFO - Epoch [245/300], Batch [0/1], Loss D: 1.2529, Loss G: 0.7278
2025-01-30 19:42:24,857 - INFO - Epoch [246/300], Batch [0/1], Loss D: 1.3084, Loss G: 0.6792
2025-01-30 19:42:24,868 - INFO - Epoch [247/300], Batch [0/1], Loss D: 1.2636, Loss G: 0.7180
2025-01-30 19:42:24,879 - INFO - Epoch [248/300], Batch [0/1], Loss D: 1.2780, Loss G: 0.7029
2025-01-30 19:42:24,891 - INFO - Epoch [249/300], Batch [0/1], Loss D: 1.2602, Loss G: 0.7209
2025-01-30 19:42:24,901 - INFO - Epoch [250/300], Batch [0/1], Loss D: 1.2735, Loss G: 0.7060
2025-01-30 19:42:24,912 - INFO - Epoch [251/300], Batch [0/1], Loss D: 1.2861, Loss G: 0.6980
2025-01-30 19:42:24,923 - INFO - Epoch [252/300], Batch [0/1], Loss D: 1.2586, Loss G: 0.7231
2025-01-30 19:42:24,933 - INFO - Epoch [253/300], Batch [0/1], Loss D: 1.2504, Loss G: 0.7282
2025-01-30 19:42:24,943 - INFO - Epoch [254/300], Batch [0/1], Loss D: 1.2595, Loss G: 0.7189
2025-01-30 19:42:24,954 - INFO - Epoch [255/300], Batch [0/1], Loss D: 1.2605, Loss G: 0.7177
2025-01-30 19:42:24,965 - INFO - Epoch [256/300], Batch [0/1], Loss D: 1.2702, Loss G: 0.7094
2025-01-30 19:42:24,977 - INFO - Epoch [257/300], Batch [0/1], Loss D: 1.2597, Loss G: 0.7186
2025-01-30 19:42:24,986 - INFO - Epoch [258/300], Batch [0/1], Loss D: 1.2923, Loss G: 0.6943
2025-01-30 19:42:24,996 - INFO - Epoch [259/300], Batch [0/1], Loss D: 1.2590, Loss G: 0.7186
2025-01-30 19:42:25,007 - INFO - Epoch [260/300], Batch [0/1], Loss D: 1.2626, Loss G: 0.7154
2025-01-30 19:42:25,017 - INFO - Epoch [261/300], Batch [0/1], Loss D: 1.2532, Loss G: 0.7241
2025-01-30 19:42:25,027 - INFO - Epoch [262/300], Batch [0/1], Loss D: 1.2743, Loss G: 0.7039
2025-01-30 19:42:25,038 - INFO - Epoch [263/300], Batch [0/1], Loss D: 1.2755, Loss G: 0.7018
2025-01-30 19:42:25,048 - INFO - Epoch [264/300], Batch [0/1], Loss D: 1.2647, Loss G: 0.7105
2025-01-30 19:42:25,058 - INFO - Epoch [265/300], Batch [0/1], Loss D: 1.2490, Loss G: 0.7260
2025-01-30 19:42:25,069 - INFO - Epoch [266/300], Batch [0/1], Loss D: 1.2462, Loss G: 0.7299
2025-01-30 19:42:25,080 - INFO - Epoch [267/300], Batch [0/1], Loss D: 1.2742, Loss G: 0.7035
2025-01-30 19:42:25,092 - INFO - Epoch [268/300], Batch [0/1], Loss D: 1.2548, Loss G: 0.7217
2025-01-30 19:42:25,101 - INFO - Epoch [269/300], Batch [0/1], Loss D: 1.2468, Loss G: 0.7287
2025-01-30 19:42:25,110 - INFO - Epoch [270/300], Batch [0/1], Loss D: 1.2293, Loss G: 0.7478
2025-01-30 19:42:25,121 - INFO - Epoch [271/300], Batch [0/1], Loss D: 1.2476, Loss G: 0.7259
2025-01-30 19:42:25,132 - INFO - Epoch [272/300], Batch [0/1], Loss D: 1.2616, Loss G: 0.7151
2025-01-30 19:42:25,144 - INFO - Epoch [273/300], Batch [0/1], Loss D: 1.2531, Loss G: 0.7197
2025-01-30 19:42:25,154 - INFO - Epoch [274/300], Batch [0/1], Loss D: 1.2508, Loss G: 0.7232
2025-01-30 19:42:25,164 - INFO - Epoch [275/300], Batch [0/1], Loss D: 1.2291, Loss G: 0.7458
2025-01-30 19:42:25,175 - INFO - Epoch [276/300], Batch [0/1], Loss D: 1.2281, Loss G: 0.7462
2025-01-30 19:42:25,185 - INFO - Epoch [277/300], Batch [0/1], Loss D: 1.2599, Loss G: 0.7171
2025-01-30 19:42:25,195 - INFO - Epoch [278/300], Batch [0/1], Loss D: 1.2644, Loss G: 0.7101
2025-01-30 19:42:25,205 - INFO - Epoch [279/300], Batch [0/1], Loss D: 1.2705, Loss G: 0.7016
2025-01-30 19:42:25,216 - INFO - Epoch [280/300], Batch [0/1], Loss D: 1.2524, Loss G: 0.7204
2025-01-30 19:42:25,227 - INFO - Epoch [281/300], Batch [0/1], Loss D: 1.2770, Loss G: 0.7037
2025-01-30 19:42:25,237 - INFO - Epoch [282/300], Batch [0/1], Loss D: 1.2538, Loss G: 0.7167
2025-01-30 19:42:25,248 - INFO - Epoch [283/300], Batch [0/1], Loss D: 1.2346, Loss G: 0.7365
2025-01-30 19:42:25,258 - INFO - Epoch [284/300], Batch [0/1], Loss D: 1.2868, Loss G: 0.6926
2025-01-30 19:42:25,268 - INFO - Epoch [285/300], Batch [0/1], Loss D: 1.2864, Loss G: 0.6885
2025-01-30 19:42:25,278 - INFO - Epoch [286/300], Batch [0/1], Loss D: 1.3016, Loss G: 0.6813
2025-01-30 19:42:25,288 - INFO - Epoch [287/300], Batch [0/1], Loss D: 1.2673, Loss G: 0.7041
2025-01-30 19:42:25,298 - INFO - Epoch [288/300], Batch [0/1], Loss D: 1.2608, Loss G: 0.7153
2025-01-30 19:42:25,309 - INFO - Epoch [289/300], Batch [0/1], Loss D: 1.2268, Loss G: 0.7455
2025-01-30 19:42:25,319 - INFO - Epoch [290/300], Batch [0/1], Loss D: 1.2520, Loss G: 0.7262
2025-01-30 19:42:25,330 - INFO - Epoch [291/300], Batch [0/1], Loss D: 1.2531, Loss G: 0.7181
2025-01-30 19:42:25,340 - INFO - Epoch [292/300], Batch [0/1], Loss D: 1.2559, Loss G: 0.7157
2025-01-30 19:42:25,354 - INFO - Epoch [293/300], Batch [0/1], Loss D: 1.2659, Loss G: 0.7068
2025-01-30 19:42:25,364 - INFO - Epoch [294/300], Batch [0/1], Loss D: 1.2599, Loss G: 0.7151
2025-01-30 19:42:25,376 - INFO - Epoch [295/300], Batch [0/1], Loss D: 1.2524, Loss G: 0.7158
2025-01-30 19:42:25,387 - INFO - Epoch [296/300], Batch [0/1], Loss D: 1.2466, Loss G: 0.7266
2025-01-30 19:42:25,399 - INFO - Epoch [297/300], Batch [0/1], Loss D: 1.2458, Loss G: 0.7222
2025-01-30 19:42:25,410 - INFO - Epoch [298/300], Batch [0/1], Loss D: 1.2701, Loss G: 0.7053
2025-01-30 19:42:25,420 - INFO - Epoch [299/300], Batch [0/1], Loss D: 1.2702, Loss G: 0.7033
2025-01-30 19:42:25,420 - INFO - GAN training finished.
2025-01-30 19:42:25,420 - INFO - Generating synthetic data...
2025-01-30 19:42:25,421 - INFO - Generating 600 synthetic samples (multiplier: 100x).
2025-01-30 19:42:25,454 - INFO - Synthetic Data Before Inverse Transform (First 5 Rows):
2025-01-30 19:42:25,454 - INFO -    numerical_column  date_column  categorical_column      cost  ID_letter  ID_number
0          0.000000     0.000000            0.125793  0.182277   0.000000   0.690458
1          0.102657     0.000000            0.720283  0.412415   2.268426   0.616481
2          0.292820     0.000000            0.541893  0.418091   1.234586   0.183049
3          0.484776     0.436684            0.000000  0.567109   0.072954   0.474215
4          0.000000     0.000000            0.000000  0.000000   0.000000   0.025527
2025-01-30 19:42:25,462 - INFO - Synthetic Data Before Inverse - Alphanumeric Column 'ID_letter' (First 5 Rows):
2025-01-30 19:42:25,463 - INFO - 0    0.000000
1    2.268426
2    1.234586
3    0.072954
4    0.000000
Name: ID_letter, dtype: float32
2025-01-30 19:42:25,463 - INFO - Synthetic Data Before Inverse - Alphanumeric Column 'ID_number' (First 5 Rows):
2025-01-30 19:42:25,464 - INFO - 0    0.690458
1    0.616481
2    0.183049
3    0.474215
4    0.025527
Name: ID_number, dtype: float32
2025-01-30 19:42:25,465 - INFO - Inverse transforming synthetic data...
2025-01-31 05:25:49,983 - INFO - Starting synthetic data generation application.
2025-01-31 05:25:49,984 - INFO - Starting synthetic data generation process...
2025-01-31 05:25:49,984 - INFO - Loading and preprocessing data...
2025-01-31 05:25:49,984 - INFO - Loading data from: C:/Users/Seiya/Desktop/SyntheticImproved/synthetic_data_generator/data/original_data.csv
2025-01-31 05:25:49,992 - INFO - Data loaded successfully. Shape: (6, 6)
2025-01-31 05:25:49,992 - INFO - Original Data (First 5 Rows):
2025-01-31 05:25:49,992 - INFO -    numerical_column date_column categorical_column                  names        cost     ID
0                 2  2023-01-01                  B  Natalie Hutchinson MD  355.036652  A1095
1                 1  2023-01-01                  A         Willie Johnson  200.449997  C3907
2                 2  2023-01-01                  A           John Sampson  200.449997  Z1095
3                 1  2023-01-01                  A         Gina Hernandez  200.449997  A0095
4                 3  2023-01-01                  A        Richard Elliott  200.449997  P6095
2025-01-31 05:25:50,065 - INFO - Column 'numerical_column' detected as numerical.
2025-01-31 05:25:50,066 - INFO - Column 'date_column' detected as name/categorical.
2025-01-31 05:25:50,067 - INFO - Column 'categorical_column' detected as name/categorical.
2025-01-31 05:25:50,067 - INFO - Column 'names' detected as the name column.
2025-01-31 05:25:50,067 - INFO - Column 'cost' detected as numerical.
2025-01-31 05:25:50,067 - INFO - Column 'ID' detected as alphanumeric.
2025-01-31 05:25:50,069 - INFO - Fitted MinMaxScaler for numerical column: numerical_column
2025-01-31 05:25:50,070 - INFO - Fitted MinMaxScaler for numerical column: cost
2025-01-31 05:25:50,071 - INFO - Fitted LabelEncoder for name/categorical column: date_column
2025-01-31 05:25:50,072 - INFO - Fitted LabelEncoder for name/categorical column: categorical_column
2025-01-31 05:25:50,077 - INFO - Columns in df_transformed: Index(['numerical_column', 'date_column', 'categorical_column', 'names',
       'cost', 'ID_letter', 'ID_number'],
      dtype='object')
2025-01-31 05:25:50,078 - INFO - Processed Data (First 5 Rows):
2025-01-31 05:25:50,079 - INFO -    numerical_column  date_column  categorical_column                  names  cost  ID_letter  ID_number
0               0.5            0                   1  Natalie Hutchinson MD   1.0          0   0.114943
1               0.0            0                   0         Willie Johnson   0.0          1   0.438161
2               0.5            0                   0           John Sampson   0.0          4   0.114943
3               0.0            0                   0         Gina Hernandez   0.0          0   0.000000
4               1.0            0                   0        Richard Elliott   0.0          3   0.689655
2025-01-31 05:25:50,095 - INFO - Training GAN model...
2025-01-31 05:25:51,609 - INFO - Starting GAN training...
2025-01-31 05:25:51,644 - INFO - Epoch [0/300], Batch [0/1], Loss D: 1.3837, Loss G: 0.6657
2025-01-31 05:25:51,656 - INFO - Epoch [1/300], Batch [0/1], Loss D: 1.3780, Loss G: 0.6703
2025-01-31 05:25:51,670 - INFO - Epoch [2/300], Batch [0/1], Loss D: 1.3789, Loss G: 0.6686
2025-01-31 05:25:51,684 - INFO - Epoch [3/300], Batch [0/1], Loss D: 1.3792, Loss G: 0.6674
2025-01-31 05:25:51,697 - INFO - Epoch [4/300], Batch [0/1], Loss D: 1.3854, Loss G: 0.6607
2025-01-31 05:25:51,709 - INFO - Epoch [5/300], Batch [0/1], Loss D: 1.3760, Loss G: 0.6688
2025-01-31 05:25:51,721 - INFO - Epoch [6/300], Batch [0/1], Loss D: 1.3756, Loss G: 0.6683
2025-01-31 05:25:51,733 - INFO - Epoch [7/300], Batch [0/1], Loss D: 1.3745, Loss G: 0.6687
2025-01-31 05:25:51,746 - INFO - Epoch [8/300], Batch [0/1], Loss D: 1.3718, Loss G: 0.6703
2025-01-31 05:25:51,759 - INFO - Epoch [9/300], Batch [0/1], Loss D: 1.3720, Loss G: 0.6693
2025-01-31 05:25:51,773 - INFO - Epoch [10/300], Batch [0/1], Loss D: 1.3719, Loss G: 0.6686
2025-01-31 05:25:51,787 - INFO - Epoch [11/300], Batch [0/1], Loss D: 1.3748, Loss G: 0.6649
2025-01-31 05:25:51,801 - INFO - Epoch [12/300], Batch [0/1], Loss D: 1.3693, Loss G: 0.6695
2025-01-31 05:25:51,813 - INFO - Epoch [13/300], Batch [0/1], Loss D: 1.3698, Loss G: 0.6682
2025-01-31 05:25:51,825 - INFO - Epoch [14/300], Batch [0/1], Loss D: 1.3680, Loss G: 0.6691
2025-01-31 05:25:51,837 - INFO - Epoch [15/300], Batch [0/1], Loss D: 1.3734, Loss G: 0.6634
2025-01-31 05:25:51,849 - INFO - Epoch [16/300], Batch [0/1], Loss D: 1.3676, Loss G: 0.6679
2025-01-31 05:25:51,862 - INFO - Epoch [17/300], Batch [0/1], Loss D: 1.3694, Loss G: 0.6654
2025-01-31 05:25:51,873 - INFO - Epoch [18/300], Batch [0/1], Loss D: 1.3647, Loss G: 0.6690
2025-01-31 05:25:51,886 - INFO - Epoch [19/300], Batch [0/1], Loss D: 1.3640, Loss G: 0.6689
2025-01-31 05:25:51,900 - INFO - Epoch [20/300], Batch [0/1], Loss D: 1.3659, Loss G: 0.6666
2025-01-31 05:25:51,914 - INFO - Epoch [21/300], Batch [0/1], Loss D: 1.3696, Loss G: 0.6619
2025-01-31 05:25:51,926 - INFO - Epoch [22/300], Batch [0/1], Loss D: 1.3689, Loss G: 0.6617
2025-01-31 05:25:51,938 - INFO - Epoch [23/300], Batch [0/1], Loss D: 1.3688, Loss G: 0.6613
2025-01-31 05:25:51,951 - INFO - Epoch [24/300], Batch [0/1], Loss D: 1.3652, Loss G: 0.6637
2025-01-31 05:25:51,962 - INFO - Epoch [25/300], Batch [0/1], Loss D: 1.3578, Loss G: 0.6701
2025-01-31 05:25:51,974 - INFO - Epoch [26/300], Batch [0/1], Loss D: 1.3674, Loss G: 0.6602
2025-01-31 05:25:51,984 - INFO - Epoch [27/300], Batch [0/1], Loss D: 1.3645, Loss G: 0.6621
2025-01-31 05:25:51,995 - INFO - Epoch [28/300], Batch [0/1], Loss D: 1.3652, Loss G: 0.6608
2025-01-31 05:25:52,005 - INFO - Epoch [29/300], Batch [0/1], Loss D: 1.3591, Loss G: 0.6657
2025-01-31 05:25:52,016 - INFO - Epoch [30/300], Batch [0/1], Loss D: 1.3616, Loss G: 0.6627
2025-01-31 05:25:52,026 - INFO - Epoch [31/300], Batch [0/1], Loss D: 1.3565, Loss G: 0.6665
2025-01-31 05:25:52,036 - INFO - Epoch [32/300], Batch [0/1], Loss D: 1.3550, Loss G: 0.6673
2025-01-31 05:25:52,047 - INFO - Epoch [33/300], Batch [0/1], Loss D: 1.3615, Loss G: 0.6605
2025-01-31 05:25:52,057 - INFO - Epoch [34/300], Batch [0/1], Loss D: 1.3598, Loss G: 0.6612
2025-01-31 05:25:52,067 - INFO - Epoch [35/300], Batch [0/1], Loss D: 1.3557, Loss G: 0.6643
2025-01-31 05:25:52,078 - INFO - Epoch [36/300], Batch [0/1], Loss D: 1.3551, Loss G: 0.6646
2025-01-31 05:25:52,088 - INFO - Epoch [37/300], Batch [0/1], Loss D: 1.3590, Loss G: 0.6597
2025-01-31 05:25:52,098 - INFO - Epoch [38/300], Batch [0/1], Loss D: 1.3532, Loss G: 0.6645
2025-01-31 05:25:52,108 - INFO - Epoch [39/300], Batch [0/1], Loss D: 1.3580, Loss G: 0.6593
2025-01-31 05:25:52,119 - INFO - Epoch [40/300], Batch [0/1], Loss D: 1.3536, Loss G: 0.6626
2025-01-31 05:25:52,130 - INFO - Epoch [41/300], Batch [0/1], Loss D: 1.3512, Loss G: 0.6644
2025-01-31 05:25:52,140 - INFO - Epoch [42/300], Batch [0/1], Loss D: 1.3499, Loss G: 0.6647
2025-01-31 05:25:52,151 - INFO - Epoch [43/300], Batch [0/1], Loss D: 1.3507, Loss G: 0.6632
2025-01-31 05:25:52,161 - INFO - Epoch [44/300], Batch [0/1], Loss D: 1.3496, Loss G: 0.6638
2025-01-31 05:25:52,172 - INFO - Epoch [45/300], Batch [0/1], Loss D: 1.3495, Loss G: 0.6629
2025-01-31 05:25:52,182 - INFO - Epoch [46/300], Batch [0/1], Loss D: 1.3499, Loss G: 0.6620
2025-01-31 05:25:52,193 - INFO - Epoch [47/300], Batch [0/1], Loss D: 1.3503, Loss G: 0.6608
2025-01-31 05:25:52,203 - INFO - Epoch [48/300], Batch [0/1], Loss D: 1.3495, Loss G: 0.6609
2025-01-31 05:25:52,214 - INFO - Epoch [49/300], Batch [0/1], Loss D: 1.3430, Loss G: 0.6663
2025-01-31 05:25:52,224 - INFO - Epoch [50/300], Batch [0/1], Loss D: 1.3521, Loss G: 0.6571
2025-01-31 05:25:52,234 - INFO - Epoch [51/300], Batch [0/1], Loss D: 1.3382, Loss G: 0.6694
2025-01-31 05:25:52,245 - INFO - Epoch [52/300], Batch [0/1], Loss D: 1.3394, Loss G: 0.6674
2025-01-31 05:25:52,255 - INFO - Epoch [53/300], Batch [0/1], Loss D: 1.3455, Loss G: 0.6610
2025-01-31 05:25:52,265 - INFO - Epoch [54/300], Batch [0/1], Loss D: 1.3436, Loss G: 0.6623
2025-01-31 05:25:52,275 - INFO - Epoch [55/300], Batch [0/1], Loss D: 1.3351, Loss G: 0.6695
2025-01-31 05:25:52,286 - INFO - Epoch [56/300], Batch [0/1], Loss D: 1.3486, Loss G: 0.6563
2025-01-31 05:25:52,296 - INFO - Epoch [57/300], Batch [0/1], Loss D: 1.3417, Loss G: 0.6620
2025-01-31 05:25:52,306 - INFO - Epoch [58/300], Batch [0/1], Loss D: 1.3437, Loss G: 0.6594
2025-01-31 05:25:52,317 - INFO - Epoch [59/300], Batch [0/1], Loss D: 1.3430, Loss G: 0.6595
2025-01-31 05:25:52,327 - INFO - Epoch [60/300], Batch [0/1], Loss D: 1.3371, Loss G: 0.6640
2025-01-31 05:25:52,339 - INFO - Epoch [61/300], Batch [0/1], Loss D: 1.3377, Loss G: 0.6629
2025-01-31 05:25:52,350 - INFO - Epoch [62/300], Batch [0/1], Loss D: 1.3356, Loss G: 0.6645
2025-01-31 05:25:52,361 - INFO - Epoch [63/300], Batch [0/1], Loss D: 1.3301, Loss G: 0.6685
2025-01-31 05:25:52,371 - INFO - Epoch [64/300], Batch [0/1], Loss D: 1.3412, Loss G: 0.6576
2025-01-31 05:25:52,382 - INFO - Epoch [65/300], Batch [0/1], Loss D: 1.3276, Loss G: 0.6697
2025-01-31 05:25:52,392 - INFO - Epoch [66/300], Batch [0/1], Loss D: 1.3329, Loss G: 0.6640
2025-01-31 05:25:52,403 - INFO - Epoch [67/300], Batch [0/1], Loss D: 1.3378, Loss G: 0.6589
2025-01-31 05:25:52,413 - INFO - Epoch [68/300], Batch [0/1], Loss D: 1.3407, Loss G: 0.6551
2025-01-31 05:25:52,423 - INFO - Epoch [69/300], Batch [0/1], Loss D: 1.3532, Loss G: 0.6434
2025-01-31 05:25:52,434 - INFO - Epoch [70/300], Batch [0/1], Loss D: 1.3343, Loss G: 0.6601
2025-01-31 05:25:52,444 - INFO - Epoch [71/300], Batch [0/1], Loss D: 1.3327, Loss G: 0.6610
2025-01-31 05:25:52,454 - INFO - Epoch [72/300], Batch [0/1], Loss D: 1.3313, Loss G: 0.6617
2025-01-31 05:25:52,465 - INFO - Epoch [73/300], Batch [0/1], Loss D: 1.3294, Loss G: 0.6627
2025-01-31 05:25:52,475 - INFO - Epoch [74/300], Batch [0/1], Loss D: 1.3282, Loss G: 0.6635
2025-01-31 05:25:52,486 - INFO - Epoch [75/300], Batch [0/1], Loss D: 1.3382, Loss G: 0.6537
2025-01-31 05:25:52,497 - INFO - Epoch [76/300], Batch [0/1], Loss D: 1.3325, Loss G: 0.6579
2025-01-31 05:25:52,507 - INFO - Epoch [77/300], Batch [0/1], Loss D: 1.3264, Loss G: 0.6627
2025-01-31 05:25:52,518 - INFO - Epoch [78/300], Batch [0/1], Loss D: 1.3269, Loss G: 0.6618
2025-01-31 05:25:52,528 - INFO - Epoch [79/300], Batch [0/1], Loss D: 1.3309, Loss G: 0.6578
2025-01-31 05:25:52,538 - INFO - Epoch [80/300], Batch [0/1], Loss D: 1.3286, Loss G: 0.6590
2025-01-31 05:25:52,549 - INFO - Epoch [81/300], Batch [0/1], Loss D: 1.3280, Loss G: 0.6590
2025-01-31 05:25:52,559 - INFO - Epoch [82/300], Batch [0/1], Loss D: 1.3228, Loss G: 0.6629
2025-01-31 05:25:52,569 - INFO - Epoch [83/300], Batch [0/1], Loss D: 1.3292, Loss G: 0.6568
2025-01-31 05:25:52,580 - INFO - Epoch [84/300], Batch [0/1], Loss D: 1.3276, Loss G: 0.6574
2025-01-31 05:25:52,590 - INFO - Epoch [85/300], Batch [0/1], Loss D: 1.3262, Loss G: 0.6581
2025-01-31 05:25:52,601 - INFO - Epoch [86/300], Batch [0/1], Loss D: 1.3299, Loss G: 0.6539
2025-01-31 05:25:52,611 - INFO - Epoch [87/300], Batch [0/1], Loss D: 1.3215, Loss G: 0.6615
2025-01-31 05:25:52,625 - INFO - Epoch [88/300], Batch [0/1], Loss D: 1.3198, Loss G: 0.6632
2025-01-31 05:25:52,636 - INFO - Epoch [89/300], Batch [0/1], Loss D: 1.3242, Loss G: 0.6575
2025-01-31 05:25:52,647 - INFO - Epoch [90/300], Batch [0/1], Loss D: 1.3160, Loss G: 0.6647
2025-01-31 05:25:52,657 - INFO - Epoch [91/300], Batch [0/1], Loss D: 1.3175, Loss G: 0.6622
2025-01-31 05:25:52,667 - INFO - Epoch [92/300], Batch [0/1], Loss D: 1.3224, Loss G: 0.6580
2025-01-31 05:25:52,679 - INFO - Epoch [93/300], Batch [0/1], Loss D: 1.3274, Loss G: 0.6522
2025-01-31 05:25:52,689 - INFO - Epoch [94/300], Batch [0/1], Loss D: 1.3359, Loss G: 0.6438
2025-01-31 05:25:52,701 - INFO - Epoch [95/300], Batch [0/1], Loss D: 1.3187, Loss G: 0.6592
2025-01-31 05:25:52,713 - INFO - Epoch [96/300], Batch [0/1], Loss D: 1.3180, Loss G: 0.6588
2025-01-31 05:25:52,725 - INFO - Epoch [97/300], Batch [0/1], Loss D: 1.3117, Loss G: 0.6644
2025-01-31 05:25:52,737 - INFO - Epoch [98/300], Batch [0/1], Loss D: 1.3266, Loss G: 0.6502
2025-01-31 05:25:52,750 - INFO - Epoch [99/300], Batch [0/1], Loss D: 1.3164, Loss G: 0.6590
2025-01-31 05:25:52,762 - INFO - Epoch [100/300], Batch [0/1], Loss D: 1.3196, Loss G: 0.6548
2025-01-31 05:25:52,774 - INFO - Epoch [101/300], Batch [0/1], Loss D: 1.3302, Loss G: 0.6453
2025-01-31 05:25:52,786 - INFO - Epoch [102/300], Batch [0/1], Loss D: 1.3080, Loss G: 0.6651
2025-01-31 05:25:52,801 - INFO - Epoch [103/300], Batch [0/1], Loss D: 1.3140, Loss G: 0.6584
2025-01-31 05:25:52,817 - INFO - Epoch [104/300], Batch [0/1], Loss D: 1.3122, Loss G: 0.6594
2025-01-31 05:25:52,830 - INFO - Epoch [105/300], Batch [0/1], Loss D: 1.3229, Loss G: 0.6499
2025-01-31 05:25:52,842 - INFO - Epoch [106/300], Batch [0/1], Loss D: 1.3057, Loss G: 0.6646
2025-01-31 05:25:52,854 - INFO - Epoch [107/300], Batch [0/1], Loss D: 1.3062, Loss G: 0.6636
2025-01-31 05:25:52,866 - INFO - Epoch [108/300], Batch [0/1], Loss D: 1.3127, Loss G: 0.6575
2025-01-31 05:25:52,879 - INFO - Epoch [109/300], Batch [0/1], Loss D: 1.3166, Loss G: 0.6536
2025-01-31 05:25:52,892 - INFO - Epoch [110/300], Batch [0/1], Loss D: 1.3108, Loss G: 0.6583
2025-01-31 05:25:52,906 - INFO - Epoch [111/300], Batch [0/1], Loss D: 1.3111, Loss G: 0.6573
2025-01-31 05:25:52,918 - INFO - Epoch [112/300], Batch [0/1], Loss D: 1.3038, Loss G: 0.6633
2025-01-31 05:25:52,930 - INFO - Epoch [113/300], Batch [0/1], Loss D: 1.3189, Loss G: 0.6495
2025-01-31 05:25:52,942 - INFO - Epoch [114/300], Batch [0/1], Loss D: 1.3174, Loss G: 0.6504
2025-01-31 05:25:52,954 - INFO - Epoch [115/300], Batch [0/1], Loss D: 1.3133, Loss G: 0.6536
2025-01-31 05:25:52,966 - INFO - Epoch [116/300], Batch [0/1], Loss D: 1.3239, Loss G: 0.6433
2025-01-31 05:25:52,979 - INFO - Epoch [117/300], Batch [0/1], Loss D: 1.3232, Loss G: 0.6441
2025-01-31 05:25:52,990 - INFO - Epoch [118/300], Batch [0/1], Loss D: 1.3145, Loss G: 0.6512
2025-01-31 05:25:53,002 - INFO - Epoch [119/300], Batch [0/1], Loss D: 1.3032, Loss G: 0.6603
2025-01-31 05:25:53,014 - INFO - Epoch [120/300], Batch [0/1], Loss D: 1.3135, Loss G: 0.6522
2025-01-31 05:25:53,027 - INFO - Epoch [121/300], Batch [0/1], Loss D: 1.3185, Loss G: 0.6464
2025-01-31 05:25:53,038 - INFO - Epoch [122/300], Batch [0/1], Loss D: 1.2974, Loss G: 0.6645
2025-01-31 05:25:53,051 - INFO - Epoch [123/300], Batch [0/1], Loss D: 1.2938, Loss G: 0.6670
2025-01-31 05:25:53,064 - INFO - Epoch [124/300], Batch [0/1], Loss D: 1.3035, Loss G: 0.6580
2025-01-31 05:25:53,076 - INFO - Epoch [125/300], Batch [0/1], Loss D: 1.2970, Loss G: 0.6638
2025-01-31 05:25:53,089 - INFO - Epoch [126/300], Batch [0/1], Loss D: 1.3171, Loss G: 0.6454
2025-01-31 05:25:53,103 - INFO - Epoch [127/300], Batch [0/1], Loss D: 1.3213, Loss G: 0.6416
2025-01-31 05:25:53,115 - INFO - Epoch [128/300], Batch [0/1], Loss D: 1.3000, Loss G: 0.6593
2025-01-31 05:25:53,127 - INFO - Epoch [129/300], Batch [0/1], Loss D: 1.2979, Loss G: 0.6608
2025-01-31 05:25:53,139 - INFO - Epoch [130/300], Batch [0/1], Loss D: 1.2942, Loss G: 0.6643
2025-01-31 05:25:53,151 - INFO - Epoch [131/300], Batch [0/1], Loss D: 1.3191, Loss G: 0.6408
2025-01-31 05:25:53,162 - INFO - Epoch [132/300], Batch [0/1], Loss D: 1.3023, Loss G: 0.6562
2025-01-31 05:25:53,174 - INFO - Epoch [133/300], Batch [0/1], Loss D: 1.3052, Loss G: 0.6524
2025-01-31 05:25:53,187 - INFO - Epoch [134/300], Batch [0/1], Loss D: 1.3068, Loss G: 0.6503
2025-01-31 05:25:53,198 - INFO - Epoch [135/300], Batch [0/1], Loss D: 1.2872, Loss G: 0.6681
2025-01-31 05:25:53,210 - INFO - Epoch [136/300], Batch [0/1], Loss D: 1.3012, Loss G: 0.6571
2025-01-31 05:25:53,222 - INFO - Epoch [137/300], Batch [0/1], Loss D: 1.2929, Loss G: 0.6616
2025-01-31 05:25:53,234 - INFO - Epoch [138/300], Batch [0/1], Loss D: 1.3010, Loss G: 0.6543
2025-01-31 05:25:53,246 - INFO - Epoch [139/300], Batch [0/1], Loss D: 1.3135, Loss G: 0.6437
2025-01-31 05:25:53,259 - INFO - Epoch [140/300], Batch [0/1], Loss D: 1.3193, Loss G: 0.6369
2025-01-31 05:25:53,270 - INFO - Epoch [141/300], Batch [0/1], Loss D: 1.2977, Loss G: 0.6569
2025-01-31 05:25:53,282 - INFO - Epoch [142/300], Batch [0/1], Loss D: 1.2987, Loss G: 0.6554
2025-01-31 05:25:53,294 - INFO - Epoch [143/300], Batch [0/1], Loss D: 1.3004, Loss G: 0.6539
2025-01-31 05:25:53,307 - INFO - Epoch [144/300], Batch [0/1], Loss D: 1.3053, Loss G: 0.6484
2025-01-31 05:25:53,320 - INFO - Epoch [145/300], Batch [0/1], Loss D: 1.2961, Loss G: 0.6596
2025-01-31 05:25:53,332 - INFO - Epoch [146/300], Batch [0/1], Loss D: 1.2937, Loss G: 0.6566
2025-01-31 05:25:53,344 - INFO - Epoch [147/300], Batch [0/1], Loss D: 1.2912, Loss G: 0.6603
2025-01-31 05:25:53,356 - INFO - Epoch [148/300], Batch [0/1], Loss D: 1.2917, Loss G: 0.6589
2025-01-31 05:25:53,368 - INFO - Epoch [149/300], Batch [0/1], Loss D: 1.2998, Loss G: 0.6526
2025-01-31 05:25:53,380 - INFO - Epoch [150/300], Batch [0/1], Loss D: 1.2888, Loss G: 0.6599
2025-01-31 05:25:53,392 - INFO - Epoch [151/300], Batch [0/1], Loss D: 1.2999, Loss G: 0.6512
2025-01-31 05:25:53,405 - INFO - Epoch [152/300], Batch [0/1], Loss D: 1.2728, Loss G: 0.6737
2025-01-31 05:25:53,419 - INFO - Epoch [153/300], Batch [0/1], Loss D: 1.2862, Loss G: 0.6627
2025-01-31 05:25:53,433 - INFO - Epoch [154/300], Batch [0/1], Loss D: 1.2921, Loss G: 0.6553
2025-01-31 05:25:53,447 - INFO - Epoch [155/300], Batch [0/1], Loss D: 1.3009, Loss G: 0.6473
2025-01-31 05:25:53,463 - INFO - Epoch [156/300], Batch [0/1], Loss D: 1.2866, Loss G: 0.6607
2025-01-31 05:25:53,478 - INFO - Epoch [157/300], Batch [0/1], Loss D: 1.2900, Loss G: 0.6571
2025-01-31 05:25:53,491 - INFO - Epoch [158/300], Batch [0/1], Loss D: 1.2912, Loss G: 0.6564
2025-01-31 05:25:53,506 - INFO - Epoch [159/300], Batch [0/1], Loss D: 1.2805, Loss G: 0.6649
2025-01-31 05:25:53,520 - INFO - Epoch [160/300], Batch [0/1], Loss D: 1.3012, Loss G: 0.6474
2025-01-31 05:25:53,534 - INFO - Epoch [161/300], Batch [0/1], Loss D: 1.2859, Loss G: 0.6595
2025-01-31 05:25:53,548 - INFO - Epoch [162/300], Batch [0/1], Loss D: 1.2979, Loss G: 0.6492
2025-01-31 05:25:53,562 - INFO - Epoch [163/300], Batch [0/1], Loss D: 1.2785, Loss G: 0.6645
2025-01-31 05:25:53,575 - INFO - Epoch [164/300], Batch [0/1], Loss D: 1.2809, Loss G: 0.6616
2025-01-31 05:25:53,589 - INFO - Epoch [165/300], Batch [0/1], Loss D: 1.3148, Loss G: 0.6326
2025-01-31 05:25:53,603 - INFO - Epoch [166/300], Batch [0/1], Loss D: 1.2774, Loss G: 0.6648
2025-01-31 05:25:53,616 - INFO - Epoch [167/300], Batch [0/1], Loss D: 1.3088, Loss G: 0.6381
2025-01-31 05:25:53,633 - INFO - Epoch [168/300], Batch [0/1], Loss D: 1.2945, Loss G: 0.6480
2025-01-31 05:25:53,650 - INFO - Epoch [169/300], Batch [0/1], Loss D: 1.2846, Loss G: 0.6586
2025-01-31 05:25:53,669 - INFO - Epoch [170/300], Batch [0/1], Loss D: 1.3001, Loss G: 0.6428
2025-01-31 05:25:53,686 - INFO - Epoch [171/300], Batch [0/1], Loss D: 1.2937, Loss G: 0.6488
2025-01-31 05:25:53,700 - INFO - Epoch [172/300], Batch [0/1], Loss D: 1.2774, Loss G: 0.6627
2025-01-31 05:25:53,715 - INFO - Epoch [173/300], Batch [0/1], Loss D: 1.2979, Loss G: 0.6446
2025-01-31 05:25:53,730 - INFO - Epoch [174/300], Batch [0/1], Loss D: 1.3164, Loss G: 0.6283
2025-01-31 05:25:53,747 - INFO - Epoch [175/300], Batch [0/1], Loss D: 1.2881, Loss G: 0.6531
2025-01-31 05:25:53,762 - INFO - Epoch [176/300], Batch [0/1], Loss D: 1.2822, Loss G: 0.6589
2025-01-31 05:25:53,779 - INFO - Epoch [177/300], Batch [0/1], Loss D: 1.2828, Loss G: 0.6552
2025-01-31 05:25:53,795 - INFO - Epoch [178/300], Batch [0/1], Loss D: 1.2930, Loss G: 0.6490
2025-01-31 05:25:53,814 - INFO - Epoch [179/300], Batch [0/1], Loss D: 1.2722, Loss G: 0.6654
2025-01-31 05:25:53,834 - INFO - Epoch [180/300], Batch [0/1], Loss D: 1.2701, Loss G: 0.6658
2025-01-31 05:25:53,847 - INFO - Epoch [181/300], Batch [0/1], Loss D: 1.2840, Loss G: 0.6539
2025-01-31 05:25:53,860 - INFO - Epoch [182/300], Batch [0/1], Loss D: 1.2771, Loss G: 0.6589
2025-01-31 05:25:53,873 - INFO - Epoch [183/300], Batch [0/1], Loss D: 1.2961, Loss G: 0.6435
2025-01-31 05:25:53,886 - INFO - Epoch [184/300], Batch [0/1], Loss D: 1.2723, Loss G: 0.6630
2025-01-31 05:25:53,899 - INFO - Epoch [185/300], Batch [0/1], Loss D: 1.2828, Loss G: 0.6537
2025-01-31 05:25:53,912 - INFO - Epoch [186/300], Batch [0/1], Loss D: 1.2986, Loss G: 0.6433
2025-01-31 05:25:53,924 - INFO - Epoch [187/300], Batch [0/1], Loss D: 1.2778, Loss G: 0.6577
2025-01-31 05:25:53,936 - INFO - Epoch [188/300], Batch [0/1], Loss D: 1.2987, Loss G: 0.6404
2025-01-31 05:25:53,948 - INFO - Epoch [189/300], Batch [0/1], Loss D: 1.3014, Loss G: 0.6391
2025-01-31 05:25:53,961 - INFO - Epoch [190/300], Batch [0/1], Loss D: 1.2803, Loss G: 0.6568
2025-01-31 05:25:53,973 - INFO - Epoch [191/300], Batch [0/1], Loss D: 1.2814, Loss G: 0.6581
2025-01-31 05:25:53,987 - INFO - Epoch [192/300], Batch [0/1], Loss D: 1.2805, Loss G: 0.6540
2025-01-31 05:25:53,999 - INFO - Epoch [193/300], Batch [0/1], Loss D: 1.2766, Loss G: 0.6576
2025-01-31 05:25:54,012 - INFO - Epoch [194/300], Batch [0/1], Loss D: 1.2640, Loss G: 0.6691
2025-01-31 05:25:54,026 - INFO - Epoch [195/300], Batch [0/1], Loss D: 1.3004, Loss G: 0.6375
2025-01-31 05:25:54,039 - INFO - Epoch [196/300], Batch [0/1], Loss D: 1.2825, Loss G: 0.6504
2025-01-31 05:25:54,052 - INFO - Epoch [197/300], Batch [0/1], Loss D: 1.2730, Loss G: 0.6591
2025-01-31 05:25:54,064 - INFO - Epoch [198/300], Batch [0/1], Loss D: 1.2576, Loss G: 0.6722
2025-01-31 05:25:54,076 - INFO - Epoch [199/300], Batch [0/1], Loss D: 1.2752, Loss G: 0.6569
2025-01-31 05:25:54,088 - INFO - Epoch [200/300], Batch [0/1], Loss D: 1.2721, Loss G: 0.6586
2025-01-31 05:25:54,100 - INFO - Epoch [201/300], Batch [0/1], Loss D: 1.2730, Loss G: 0.6582
2025-01-31 05:25:54,113 - INFO - Epoch [202/300], Batch [0/1], Loss D: 1.2706, Loss G: 0.6622
2025-01-31 05:25:54,126 - INFO - Epoch [203/300], Batch [0/1], Loss D: 1.2702, Loss G: 0.6609
2025-01-31 05:25:54,138 - INFO - Epoch [204/300], Batch [0/1], Loss D: 1.3064, Loss G: 0.6326
2025-01-31 05:25:54,151 - INFO - Epoch [205/300], Batch [0/1], Loss D: 1.2734, Loss G: 0.6557
2025-01-31 05:25:54,164 - INFO - Epoch [206/300], Batch [0/1], Loss D: 1.2677, Loss G: 0.6604
2025-01-31 05:25:54,176 - INFO - Epoch [207/300], Batch [0/1], Loss D: 1.2696, Loss G: 0.6597
2025-01-31 05:25:54,189 - INFO - Epoch [208/300], Batch [0/1], Loss D: 1.2542, Loss G: 0.6720
2025-01-31 05:25:54,200 - INFO - Epoch [209/300], Batch [0/1], Loss D: 1.2715, Loss G: 0.6607
2025-01-31 05:25:54,211 - INFO - Epoch [210/300], Batch [0/1], Loss D: 1.3015, Loss G: 0.6338
2025-01-31 05:25:54,221 - INFO - Epoch [211/300], Batch [0/1], Loss D: 1.2510, Loss G: 0.6748
2025-01-31 05:25:54,232 - INFO - Epoch [212/300], Batch [0/1], Loss D: 1.2682, Loss G: 0.6622
2025-01-31 05:25:54,242 - INFO - Epoch [213/300], Batch [0/1], Loss D: 1.2768, Loss G: 0.6541
2025-01-31 05:25:54,253 - INFO - Epoch [214/300], Batch [0/1], Loss D: 1.2667, Loss G: 0.6620
2025-01-31 05:25:54,264 - INFO - Epoch [215/300], Batch [0/1], Loss D: 1.2844, Loss G: 0.6461
2025-01-31 05:25:54,274 - INFO - Epoch [216/300], Batch [0/1], Loss D: 1.2868, Loss G: 0.6463
2025-01-31 05:25:54,284 - INFO - Epoch [217/300], Batch [0/1], Loss D: 1.2889, Loss G: 0.6434
2025-01-31 05:25:54,294 - INFO - Epoch [218/300], Batch [0/1], Loss D: 1.2656, Loss G: 0.6607
2025-01-31 05:25:54,305 - INFO - Epoch [219/300], Batch [0/1], Loss D: 1.2861, Loss G: 0.6436
2025-01-31 05:25:54,315 - INFO - Epoch [220/300], Batch [0/1], Loss D: 1.2546, Loss G: 0.6685
2025-01-31 05:25:54,325 - INFO - Epoch [221/300], Batch [0/1], Loss D: 1.2626, Loss G: 0.6611
2025-01-31 05:25:54,335 - INFO - Epoch [222/300], Batch [0/1], Loss D: 1.2722, Loss G: 0.6550
2025-01-31 05:25:54,346 - INFO - Epoch [223/300], Batch [0/1], Loss D: 1.2826, Loss G: 0.6432
2025-01-31 05:25:54,356 - INFO - Epoch [224/300], Batch [0/1], Loss D: 1.2884, Loss G: 0.6386
2025-01-31 05:25:54,367 - INFO - Epoch [225/300], Batch [0/1], Loss D: 1.2537, Loss G: 0.6686
2025-01-31 05:25:54,379 - INFO - Epoch [226/300], Batch [0/1], Loss D: 1.2982, Loss G: 0.6344
2025-01-31 05:25:54,389 - INFO - Epoch [227/300], Batch [0/1], Loss D: 1.2919, Loss G: 0.6402
2025-01-31 05:25:54,400 - INFO - Epoch [228/300], Batch [0/1], Loss D: 1.2630, Loss G: 0.6586
2025-01-31 05:25:54,411 - INFO - Epoch [229/300], Batch [0/1], Loss D: 1.2663, Loss G: 0.6572
2025-01-31 05:25:54,421 - INFO - Epoch [230/300], Batch [0/1], Loss D: 1.2833, Loss G: 0.6439
2025-01-31 05:25:54,432 - INFO - Epoch [231/300], Batch [0/1], Loss D: 1.2608, Loss G: 0.6612
2025-01-31 05:25:54,443 - INFO - Epoch [232/300], Batch [0/1], Loss D: 1.2903, Loss G: 0.6389
2025-01-31 05:25:54,454 - INFO - Epoch [233/300], Batch [0/1], Loss D: 1.2381, Loss G: 0.6802
2025-01-31 05:25:54,464 - INFO - Epoch [234/300], Batch [0/1], Loss D: 1.2547, Loss G: 0.6690
2025-01-31 05:25:54,475 - INFO - Epoch [235/300], Batch [0/1], Loss D: 1.2717, Loss G: 0.6506
2025-01-31 05:25:54,485 - INFO - Epoch [236/300], Batch [0/1], Loss D: 1.2870, Loss G: 0.6398
2025-01-31 05:25:54,496 - INFO - Epoch [237/300], Batch [0/1], Loss D: 1.2637, Loss G: 0.6575
2025-01-31 05:25:54,506 - INFO - Epoch [238/300], Batch [0/1], Loss D: 1.2679, Loss G: 0.6565
2025-01-31 05:25:54,516 - INFO - Epoch [239/300], Batch [0/1], Loss D: 1.2569, Loss G: 0.6631
2025-01-31 05:25:54,527 - INFO - Epoch [240/300], Batch [0/1], Loss D: 1.2745, Loss G: 0.6476
2025-01-31 05:25:54,537 - INFO - Epoch [241/300], Batch [0/1], Loss D: 1.2471, Loss G: 0.6701
2025-01-31 05:25:54,547 - INFO - Epoch [242/300], Batch [0/1], Loss D: 1.2614, Loss G: 0.6576
2025-01-31 05:25:54,556 - INFO - Epoch [243/300], Batch [0/1], Loss D: 1.2740, Loss G: 0.6451
2025-01-31 05:25:54,567 - INFO - Epoch [244/300], Batch [0/1], Loss D: 1.2354, Loss G: 0.6818
2025-01-31 05:25:54,577 - INFO - Epoch [245/300], Batch [0/1], Loss D: 1.2868, Loss G: 0.6393
2025-01-31 05:25:54,587 - INFO - Epoch [246/300], Batch [0/1], Loss D: 1.2747, Loss G: 0.6443
2025-01-31 05:25:54,598 - INFO - Epoch [247/300], Batch [0/1], Loss D: 1.2546, Loss G: 0.6618
2025-01-31 05:25:54,608 - INFO - Epoch [248/300], Batch [0/1], Loss D: 1.2725, Loss G: 0.6474
2025-01-31 05:25:54,618 - INFO - Epoch [249/300], Batch [0/1], Loss D: 1.2409, Loss G: 0.6763
2025-01-31 05:25:54,629 - INFO - Epoch [250/300], Batch [0/1], Loss D: 1.2758, Loss G: 0.6458
2025-01-31 05:25:54,639 - INFO - Epoch [251/300], Batch [0/1], Loss D: 1.2539, Loss G: 0.6630
2025-01-31 05:25:54,650 - INFO - Epoch [252/300], Batch [0/1], Loss D: 1.2682, Loss G: 0.6518
2025-01-31 05:25:54,661 - INFO - Epoch [253/300], Batch [0/1], Loss D: 1.2929, Loss G: 0.6332
2025-01-31 05:25:54,671 - INFO - Epoch [254/300], Batch [0/1], Loss D: 1.2699, Loss G: 0.6522
2025-01-31 05:25:54,690 - INFO - Epoch [255/300], Batch [0/1], Loss D: 1.2902, Loss G: 0.6347
2025-01-31 05:25:54,701 - INFO - Epoch [256/300], Batch [0/1], Loss D: 1.2865, Loss G: 0.6363
2025-01-31 05:25:54,711 - INFO - Epoch [257/300], Batch [0/1], Loss D: 1.2597, Loss G: 0.6558
2025-01-31 05:25:54,721 - INFO - Epoch [258/300], Batch [0/1], Loss D: 1.2377, Loss G: 0.6770
2025-01-31 05:25:54,732 - INFO - Epoch [259/300], Batch [0/1], Loss D: 1.2256, Loss G: 0.6881
2025-01-31 05:25:54,741 - INFO - Epoch [260/300], Batch [0/1], Loss D: 1.2940, Loss G: 0.6307
2025-01-31 05:25:54,752 - INFO - Epoch [261/300], Batch [0/1], Loss D: 1.2960, Loss G: 0.6300
2025-01-31 05:25:54,762 - INFO - Epoch [262/300], Batch [0/1], Loss D: 1.2955, Loss G: 0.6450
2025-01-31 05:25:54,773 - INFO - Epoch [263/300], Batch [0/1], Loss D: 1.2767, Loss G: 0.6441
2025-01-31 05:25:54,783 - INFO - Epoch [264/300], Batch [0/1], Loss D: 1.2381, Loss G: 0.6739
2025-01-31 05:25:54,793 - INFO - Epoch [265/300], Batch [0/1], Loss D: 1.2775, Loss G: 0.6504
2025-01-31 05:25:54,805 - INFO - Epoch [266/300], Batch [0/1], Loss D: 1.2725, Loss G: 0.6511
2025-01-31 05:25:54,818 - INFO - Epoch [267/300], Batch [0/1], Loss D: 1.3192, Loss G: 0.6150
2025-01-31 05:25:54,831 - INFO - Epoch [268/300], Batch [0/1], Loss D: 1.2716, Loss G: 0.6460
2025-01-31 05:25:54,848 - INFO - Epoch [269/300], Batch [0/1], Loss D: 1.2239, Loss G: 0.6862
2025-01-31 05:25:54,860 - INFO - Epoch [270/300], Batch [0/1], Loss D: 1.2576, Loss G: 0.6572
2025-01-31 05:25:54,874 - INFO - Epoch [271/300], Batch [0/1], Loss D: 1.2659, Loss G: 0.6539
2025-01-31 05:25:54,887 - INFO - Epoch [272/300], Batch [0/1], Loss D: 1.2677, Loss G: 0.6468
2025-01-31 05:25:54,899 - INFO - Epoch [273/300], Batch [0/1], Loss D: 1.2594, Loss G: 0.6554
2025-01-31 05:25:54,910 - INFO - Epoch [274/300], Batch [0/1], Loss D: 1.2479, Loss G: 0.6681
2025-01-31 05:25:54,922 - INFO - Epoch [275/300], Batch [0/1], Loss D: 1.2781, Loss G: 0.6547
2025-01-31 05:25:54,933 - INFO - Epoch [276/300], Batch [0/1], Loss D: 1.2580, Loss G: 0.6579
2025-01-31 05:25:54,945 - INFO - Epoch [277/300], Batch [0/1], Loss D: 1.2754, Loss G: 0.6459
2025-01-31 05:25:54,957 - INFO - Epoch [278/300], Batch [0/1], Loss D: 1.2754, Loss G: 0.6464
2025-01-31 05:25:54,969 - INFO - Epoch [279/300], Batch [0/1], Loss D: 1.2817, Loss G: 0.6400
2025-01-31 05:25:54,982 - INFO - Epoch [280/300], Batch [0/1], Loss D: 1.2817, Loss G: 0.6389
2025-01-31 05:25:54,997 - INFO - Epoch [281/300], Batch [0/1], Loss D: 1.2508, Loss G: 0.6668
2025-01-31 05:25:55,011 - INFO - Epoch [282/300], Batch [0/1], Loss D: 1.2904, Loss G: 0.6273
2025-01-31 05:25:55,024 - INFO - Epoch [283/300], Batch [0/1], Loss D: 1.2662, Loss G: 0.6500
2025-01-31 05:25:55,036 - INFO - Epoch [284/300], Batch [0/1], Loss D: 1.2894, Loss G: 0.6388
2025-01-31 05:25:55,048 - INFO - Epoch [285/300], Batch [0/1], Loss D: 1.2482, Loss G: 0.6653
2025-01-31 05:25:55,061 - INFO - Epoch [286/300], Batch [0/1], Loss D: 1.2446, Loss G: 0.6681
2025-01-31 05:25:55,073 - INFO - Epoch [287/300], Batch [0/1], Loss D: 1.2772, Loss G: 0.6463
2025-01-31 05:25:55,085 - INFO - Epoch [288/300], Batch [0/1], Loss D: 1.2892, Loss G: 0.6463
2025-01-31 05:25:55,102 - INFO - Epoch [289/300], Batch [0/1], Loss D: 1.2422, Loss G: 0.6683
2025-01-31 05:25:55,119 - INFO - Epoch [290/300], Batch [0/1], Loss D: 1.2673, Loss G: 0.6493
2025-01-31 05:25:55,135 - INFO - Epoch [291/300], Batch [0/1], Loss D: 1.2684, Loss G: 0.6485
2025-01-31 05:25:55,146 - INFO - Epoch [292/300], Batch [0/1], Loss D: 1.2344, Loss G: 0.6759
2025-01-31 05:25:55,157 - INFO - Epoch [293/300], Batch [0/1], Loss D: 1.2791, Loss G: 0.6400
2025-01-31 05:25:55,168 - INFO - Epoch [294/300], Batch [0/1], Loss D: 1.2695, Loss G: 0.6502
2025-01-31 05:25:55,179 - INFO - Epoch [295/300], Batch [0/1], Loss D: 1.3122, Loss G: 0.6150
2025-01-31 05:25:55,190 - INFO - Epoch [296/300], Batch [0/1], Loss D: 1.2432, Loss G: 0.6686
2025-01-31 05:25:55,201 - INFO - Epoch [297/300], Batch [0/1], Loss D: 1.2960, Loss G: 0.6321
2025-01-31 05:25:55,211 - INFO - Epoch [298/300], Batch [0/1], Loss D: 1.2948, Loss G: 0.6438
2025-01-31 05:25:55,221 - INFO - Epoch [299/300], Batch [0/1], Loss D: 1.2466, Loss G: 0.6655
2025-01-31 05:25:55,222 - INFO - GAN training finished.
2025-01-31 05:25:55,222 - INFO - Generating synthetic data...
2025-01-31 05:25:55,223 - INFO - Generating 600 synthetic samples (multiplier: 100x).
2025-01-31 05:25:55,254 - INFO - Synthetic Data Before Inverse Transform (First 5 Rows):
2025-01-31 05:25:55,255 - INFO -    numerical_column  date_column  categorical_column      cost  ID_letter  ID_number
0          0.215690     0.177021            0.059742  0.000000   0.000000   0.540156
1          0.000000     0.000000            0.000000  0.000000   1.134576   0.081446
2          0.133403     0.000000            0.268594  0.000000   0.858606   0.000000
3          0.000000     0.200667            0.458610  0.996853   1.458733   0.044998
4          0.058323     0.000000            0.000000  0.136997   0.739641   0.000000
2025-01-31 05:25:55,263 - INFO - Synthetic Data Before Inverse - Alphanumeric Column 'ID_letter' (First 5 Rows):
2025-01-31 05:25:55,263 - INFO - 0    0.000000
1    1.134576
2    0.858606
3    1.458733
4    0.739641
Name: ID_letter, dtype: float32
2025-01-31 05:25:55,264 - INFO - Synthetic Data Before Inverse - Alphanumeric Column 'ID_number' (First 5 Rows):
2025-01-31 05:25:55,266 - INFO - 0    0.540156
1    0.081446
2    0.000000
3    0.044998
4    0.000000
Name: ID_number, dtype: float32
2025-01-31 05:25:55,267 - INFO - Inverse transforming synthetic data...
2025-01-31 05:25:57,533 - INFO - Synthetic Data After Inverse Transform (First 5 Rows):
2025-01-31 05:25:57,534 - INFO -    numerical_column date_column categorical_column        cost     ID              names
0                 1  2023-01-01                  A  200.449997  A4794        Bobby Moore
1                 1  2023-01-01                  A  200.449997   C804     William Pierce
2                 1  2023-01-01                  A  200.449997    C95       Austin Green
3                 1  2023-01-01                  A  354.550171   C486  Michael Henderson
4                 1  2023-01-01                  A  221.627884    D95      Monica Conway
2025-01-31 05:25:57,541 - INFO - Synthetic Data After Inverse - Alphanumeric Column 'ID' (First 5 Rows):
2025-01-31 05:25:57,542 - INFO - 0    A4794
1     C804
2      C95
3     C486
4      D95
Name: ID, dtype: object
2025-01-31 05:25:57,543 - INFO - Saving synthetic data to: data/synthetic_data.csv
2025-01-31 05:25:57,548 - INFO - Synthetic data generation complete.
2025-01-31 05:25:57,549 - INFO - Synthetic data generation process finished successfully.
